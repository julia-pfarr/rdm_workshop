{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Research Data Management (RDM)","text":""},{"location":"#overview","title":"Overview","text":"<p>This RDM workshop is designed to give a generic overview of RDM principles and practices. In most research fields there are field specific and community driven standards and practices, especially for file organization and metadata. This workshop does not address this field specific standards as it was designed for a broad audience from different research areas. </p>"},{"location":"#topics","title":"Topics \ud83d\udca1","text":"<p>Under the following sections you'll find the <code>objectives</code> for each of the topics we'll discuss during this workshop and further <code>materials/readings</code>.</p> <ul> <li>Introduction to RDM </li> <li>The Open Science Framework (OSF) </li> <li>Research Data Management Plan (RDMP) </li> <li>Pre-registration </li> <li>Project and Data Organization </li> <li>Metadata </li> <li>Version Control </li> <li>Data Storage and Sharing </li> <li>Copyright and Licenses</li> </ul>"},{"location":"#optionalreadingfurther-materials","title":"optional/reading/further materials","text":"<ul> <li>The Turing Way Handbook</li> <li>UK Data Service</li> <li>HeFDI workshops, materials, and publications</li> <li>forschungsdaten.info</li> <li>OHBM Open Science Educationals</li> <li>NOWA Blog and tutorials</li> <li>NOWA School: a one week school on RDM and reproducible code using Python</li> <li>Brainhack School Modules</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>My work on this workshop was enabled through my work and training as a Data Steward in the SFB 135 \"Cardinal Mechanisms of Perception\", infrastructure project NOWA.</p>"},{"location":"copyright_licenses/","title":"Copyright and Licenses","text":"<p>'Intellectual Property (IP)' law is a complex subject. However some understanding of it is important for anyone producing creative works governed by it including software, datasets, graphics and more. This is true irrespective of the nature of your project: Closed commercial projects building on open tooling; Commercial projects maintaining an open resource; Open community driven and/or non-profit projects. Each of these may need to make slightly different licensing choices from the beginning of their projects to be compatible with their goals. Decisions about licencing made at the inception of a project can have long-lasting and significant ramifications. The choices that you make about how your work is licensed shape who can and cannot legally use your work and for what purpose.</p> <p>Many of the concepts which apply to the licensing of software, data, AI/ML models, hardware and other creative works such as visuals share common attributes and concepts which will be covered here.</p> <p>Intellectual property is an umbrella term that refers to a number of distinct areas of law, primarily these three:</p> <ul> <li>Copyright</li> <li>Patent (not discussed here)</li> <li>Trademark (not discussed here)</li> </ul> <p>What these have in common is the attempt to extend property rights to intangible goods, meaning their use by others can be prevented or licensed. Governments with such laws effectively create a limited grant of monopoly over these goods for their creators, and other holders of these rights. This is generally done with the ostensible intent to incentivise the creation and improvement of such goods, but can in practice result in perverse incentives which fail to do so.</p> <p>Note</p> <p>It is important to consider that copyright, licenses, and patents are all legal concepts. As such, they are subject to what the law prescribes, which may change over time and space. Simply put, different countries have different laws, and follow different procedures with regard to enforcing them. The content provided here is broadly based on American and European law and legal traditions. It might not be applicable - might even be contra indicated - or relevant in your particular context. However most nations are signatories to international treaty agreements which somewhat harmonise these laws notably the Berne Convention, the TRIPS Agreement, and others under the World Intellectual Property Organization (WIPO). Whilst international efforts have sought to harmonize copyright enforcement, the real world is a messy place.</p> <p>Another note</p> <p>Good legal advice is timely, specific, and given by an expert; this chapter is none of these. It was written by engineers &amp; scientists, not by lawyers, and it is a heavily simplified overview of a very complex field. The intent is to give you an overview of the basics so that you will know when to check whether something you want to do has potential legal ramifications. Do not make any important decisions based solely on the contents of this chapter.</p> <p>So do not take the descriptions provided or viewpoints shared as legal advice, they are not that. This document is not intended to be used in that manner. Consult a legal expert to provide actual legal advice for your case.</p>"},{"location":"copyright_licenses/#copyright","title":"Copyright","text":"<p>(Research) Data per se are not protected by copyright. A copyright only applies to a \u201cpersonal intellectual creation\u201d (\u00a7 Abs. 2 UrhG) (=criterium individuality). This is not true for data, as data is seen as facts. This means for data, only the structure of it can be protected by copyright (\u00a7 4 UrhG). For example: The initial table with the data is not protected by copyright, but if the researchers invested comprehensive re-structuring of the data (e.g., by adding new derivatives of the data after pre-processing), a copyright hold by the creator is valid. The same is true for metadata. Beholder of the copyright is always the creator, i.e., the researcher. If the dataset was created in collaboration, all collaborators share the same copyright.</p> <p>Professors are not obligated to share the created datasets with the university as it is implied that the research work of professors are not bound by instructions of the university (\u201cnicht weisungsgebunden\u201d). It is different for students and other scientific staff as their work is usually bound to instructions of the university (\u201cweisungsgebunden\u201d). This means, that the university holds usage rights for datasets specifically created for theses (Bachelor, Master, Dissertation) by students. Third-party funders such as the DFG do not have any usage rights, but expect the researchers to take care of this by themselves. There are no regulations to which extend researchers should share their data but they highly recommend to share as much as possible.</p> <p>By default, if you make a work publicly available, you retain the copyright to that work and all rights that this gives you over it. Anyone wishing to re-use that work must seek to license the right to do so from you, or open themselves to the possibility of a lawsuit for infringing on your copyright.</p>"},{"location":"copyright_licenses/#optionalreadingfurther-materials","title":"optional/reading/further materials","text":"<ul> <li>Urheberrecht in der Wissenschaft (Copyright law in academia) (insb. 24ff)</li> <li>Datenschutzrecht (Data protection law)</li> <li>Urheberrecht (Copyright law)</li> <li>Rechtsfragen bei Open Science (Legal Issues with Open Science)</li> <li>Wem \"geh\u00f6ren\" Forschungsdaten? </li> <li>Gutachten zu den rechtlichen Rahmenbedingungen des Forschungsdatenmanagements (Expert opinion on the legal framework of Research Data Management)</li> <li>Fact Sheet Personal Data</li> <li>Asking for permission</li> <li>Ethics and data protection</li> <li>Rechtliche Rahmenbedingungen des Forschungsdatenmanagements</li> <li>Data Protection Decision Tree</li> <li>FDM Recht</li> </ul>"},{"location":"copyright_licenses/#licenses","title":"Licenses","text":""},{"location":"copyright_licenses/#what-are-usage-restricting-licenses","title":"What are 'Usage Restricting' Licenses?","text":"<p>Usage restricting licenses seek to affirmatively protect users or others affected by the use of the work by placing specific restrictions on its use. This curtails freedom 0, the freedom to use software 'for any purpose' and prohibiting the use of the software, or other system, for unethical purposes. Both 'Ethical source' &amp; 'Responsible AI' Licenses are examples of this approach and seek to place restrictions on the uses to which the licensees can put the software or machine learning systems licensed in this fashion. Consequently, these licenses by the classical definitions of free and open source software from the FSF and OSI would not be considered free or open source licenses. They do however generally resemble them in the other three criteria of the definition. Their merits versus conventional open source licenses have been the subject of some debate, and their adoption has thus far been relatively limited.</p> <p>Even an attribution requirement (the BY in CC-BY) can in some cases be considered a usage restriction. For example the Debian project found the Common Public Attribution License (CPAL) to be incompatible their free-software guidelines for this reason whilst it is approved by the Open Source Initiative. In the case of academic works attribution requirements can serve to re-enforce the citation convention with the force of copyright law.</p>"},{"location":"copyright_licenses/#where-to-find-open-licenses-for-different-types-of-work","title":"Where to find open licenses for different types of work","text":"<ul> <li>Code<ul> <li>The Open Source Initiaitive (OSI) maintains a list of approved licenses open source licenses</li> <li>Free Software Foundation maintains a list of GPL-Compatible Free Software Licenses<ul> <li>GNU/FSF recomendations</li> </ul> </li> <li>choosealicense.com provides a tool to guide you through the license choice project.</li> <li>Organisation for ethical source maintains a list of ethical source licenses</li> </ul> </li> <li>Prose, Images, Audio, Video, Datasets, and similar<ul> <li>Creative Commons (CC)<ul> <li>Creative Commons License Chooser</li> </ul> </li> </ul> </li> <li>Machine Learning (ML) / artificial inteligence (AI) systems<ul> <li>Creative commons and Software licenses can be applied to different parts of ML/AI systems, CC to training data and weights, software licenses to code used in training / depoyment.</li> <li>Responsible AI Licenses (RAIL)</li> </ul> </li> </ul>"},{"location":"copyright_licenses/#licencing-enforcement","title":"Licencing enforcement","text":"<p>There have been a number of successful legal cases that have been brought in defence of the terms of copyleft licenses obliging the parties abusing the terms of these licenses to appropriately release their code. But this can be hard to discover, as it is not immediately obvious if copyleft code has been used from looking at a black box proprietary end product.</p> <p>Organisations which take legal action in defence of free software, and which can provide information and resources for anyone else seeking to do the same, include:</p> <ul> <li>Software Freedom concervancy</li> <li>Software Freedom Law Centre</li> <li>FSF - licensing and compliance</li> <li>Free Software Foundation Europe (FSFe) - legal work</li> <li>Electronic Fontiers Foundation - legal cases</li> </ul>"},{"location":"copyright_licenses/#contributor-license-agreements","title":"Contributor license Agreements","text":"<p>The holder of the copyright on a copyleft project can still re-license that project or dual-license that project under a different license, for example to grant exclusive rights to commercially distribute that project with proprietary extensions or to make future versions proprietary. In a large community developed project, this would require the consent of all contributors, as they each own the copyright to their contributions. To get around this, some copyleft projects developed by companies that commercially license proprietary extensions to these projects ask their contributors to sign contributor license agreements (CLAs) which may assign the contributor's copyright to the company, or include other provisions so that they can legally dual-license the project.</p>"},{"location":"copyright_licenses/#how-and-where-to-add-licenses","title":"How and where to add licenses","text":"<p>Wherever you share your project it is likely to be organised in a heirarchy of directories, place a plain text file containing the license in the top level directoty of your project. If it is a git project for example that is shared on a git forge like github or gitlab, using a standard file name like <code>LICENSE</code> will allow your license to be picked up the the host and displayed on your project. If the license that you have used has a standarised short name from SPDX then this will be displayed as a small icon on your projects home page by these hosts. It can also be useful to include license information in the form of standard strings at the top of each text file in your project. There are useful tools which automate this available from REUSE a project from the FSFe which developed the spec. This is especially true if your project contains material that is licensed in multiple different ways or a part of your project is being used in someone else's which uses a different (compatible) license.</p> <p>Task</p> <p>Go to the Creative Commons License Chooser and select a license. Then, go to your OSF project and add a LICENSE file.  </p>"},{"location":"introduction/","title":"Introduction","text":"<p>Objectives\ud83d\udccd</p> <ul> <li>What Research Data Management (RDM) means and why it is important</li> <li>The FAIR principles</li> <li>The Research Life Cycle</li> <li>RDM &amp; Open Science</li> <li>Overview of RDM components</li> </ul>"},{"location":"introduction/#summary","title":"Summary","text":"Research Data Management: making it possible to retreive data from last year. The Turing Way project illustration by Scriberia. Reused under a CC-BY 4.0 license. Original version on Zenodo. http://doi.org/10.5281/zenodo.3695300. <p>Research Data Management (RDM) entails practices of storing, annotating, and re-using research data. We are all doing it every day: we all store, sort, and annotate our data in our own way, namely in the way we've always been doing it and how we feel the most comfortable with. This might be sufficient, you think - at least that's how it feels in the moment. However, leaving RDM to be an individual issue results in several restrictions for the individual as well as for science per se. When getting back to your data after some time (1) and try to find, read, and understand it, you might realize that your way of storing, sorting, and annotating your data isn't so intuitive after all. </p> <ol> <li>e.g., you've done your work and submitted a manuscript, got reviews which want you to give some more information on your data/analyses or run some more analyses.</li> </ol> <p>Do you find everything you need in a short time? Do you understand the different folders and files instantly? Does your code still run without errors? Do you know which was the latest version of your project? Maybe...or maybe not? So, if you yourself, the master of your data, are unable to confidently reuse your data...what about others? </p> <p>The field of RDM provides us with recommendations, schemes, and standards which eventually will save all of us a lot of time and memory while also fostering collaboration, preventing data loss, and improving the overall quality of science. </p>"},{"location":"introduction/#the-research-data-lifecycle","title":"The Research Data Lifecycle","text":"<p>RDM techniques are aiming to support our research work in all stages of the so-called Research Data Lifecycle:</p> Research Data Lifecycle. The Turing Way project illustration by Scriberia. Reused under a CC-BY 4.0 license. Original version on Zenodo. http://doi.org/10.5281/zenodo.3695300. <p>Depending on your career stage, you might not have been confronted with every single stage of this cycle yet. However, each step of the research life cycle needs to be considered for RDM. Developing a research idea is exciting and fun but your idea will only come true when you care just as much about the other stages in the research data lifecycle. Here's what you will encounter during the different stages:</p> Planning &amp; DesignCollecting dataProcessing &amp; analysing dataPublishing &amp; sharing dataPreserving dataReusing data <ul> <li>design research</li> <li>plan data management</li> <li>plan consent for sharing</li> <li>plan data collecting, processing, protocols, and templates</li> <li>explore existing data sources</li> </ul> <ul> <li>collect data</li> <li>capture data with metadata</li> <li>acquire existing third party data </li> </ul> <ul> <li>enter, digitize, transcribe, and translate data</li> <li>check, validate, clean, anonymize</li> <li>derive data</li> <li>describe and document data</li> <li>manage and store data</li> <li>analyse and interpret data</li> <li>produce research outputs</li> <li>cite data sources</li> </ul> <ul> <li>establish copyright</li> <li>create user documentation</li> <li>create discovery metadata</li> <li>select appropriate access to data</li> <li>publish/ share data</li> <li>promote data</li> </ul> <ul> <li>migrate data to best format/media</li> <li>store and back up data</li> <li>create preservation documentation</li> <li>preserve and curate data </li> </ul> <ul> <li>conduct secondary analysis</li> <li>undertake follow-up research</li> <li>conduct research reviews</li> <li>scrutinize findings</li> <li>use data for teaching and learning</li> </ul>"},{"location":"introduction/#the-fair-principles-and-practices","title":"The FAIR principles and practices","text":"<p>Behind every RDM technique stands the FAIR principles. FAIR means that research and data should be</p> <ul> <li>Findable: Annotating your research and data with proper Metadata goes a long way. This enables to search for specific keywords and find the respective research items.</li> <li>Accessible: Data users must know how to access the data. This starts with simple metadata about hard- and software requirements and involves information about potential authentication and authorisation procedures. </li> <li>Interoperable: Reuse of data and collaboration can only work if the data is provided in a way that it can be easily integrated with other data or easily operated with other applications. </li> <li>Reusable: Again, sufficient metadata and requirements for using your data in different settings have to be given to make your data reusable. </li> </ul> <p>Here's what you should know about FAIR:</p> <ul> <li>By making your data FAIR you expand the life-cycle of your data and contribute to sustainability in the digital world, which brings benefits for your own research as well as the whole science community.</li> <li>FAIR does not equal open. Following the FAIR principles does not necessarily mean that you have to share your data with everyone. Your data can meet the FAIR principles but still be shared under certain restrictions.</li> <li>The intention of the FAIR principles is not to have everybody apply all the principles to all their data perfectly right away. Implementing just a few of the recommended steps helps you and the science community to get one step further toward open science. The important thing is simply to start somewhere to get used to practicing the FAIR principles on your data regularly. Step by step.</li> <li>The FAIR principles are guidelines, not a standard.</li> </ul>"},{"location":"introduction/#rdm-open-science","title":"RDM &amp; Open Science","text":"Private Data. The Turing Way project illustration by Scriberia. Reused under a CC-BY 4.0 license. Original version on Zenodo. http://doi.org/10.5281/zenodo.3695300. <p>Personally, I think that there are two misunderstandings when people think of Open Science. </p> <p>First, just putting everything you have in your project folder on some repository and sharing it with the world does not make your research \"open\". If your data isn't FAIR, people won't be able to derive any more information from it than you've given in a conventional paper. People might not be able to open your files, don't know what your data is about without sufficient metadata, cannot run your code due to proprietary software you used etc. etc. </p> <p>Second, making your research open does not mean that you have to share everything you have with everyone in the world. Sometimes this is simply not possible, for example when it comes to personal data. This is a good thing, people deserve their right to having their private data protected. What you can still do then though, is to share comprehensive metadata to let people know what your data is about and grant them authorized access (i.e., Data User Agreements). If you then additionally share your code, other researchers can still reuse your resources for reproducibility and/or replicability purposes. </p> <p>Thus, RDM and Open Science go hand in hand: without proper RDM your research won't be open. </p> <p>Applying RDM standards to your work and contributing to Open Science benefits you by increasing your data quality, accountability, reputation in the science community, time efficacy, visibility &amp; transparency and many more:</p> Reasons for sharing your work. The Turing Way project illustration by Scriberia. Reused under a CC-BY 4.0 license. Original version on Zenodo. http://doi.org/10.5281/zenodo.3695300."},{"location":"introduction/#rdm-components","title":"RDM components","text":"<p>As described above, RDM aims to support you in all stages of the research data lifecycle. However, there are some components of RDM which are especially important and which we will get to know in more detail in this workshop:</p> <ul> <li>Planning &amp; Design: Making a Research Data Management Plan (rdmp) and pre-registering your study</li> <li>Data collection: Organizing your data and annotating it with metadata</li> <li>Analysing data: using open source software and version control as well as ensuring reproducibility of your research</li> <li>Publishing &amp; sharing data: Popular repositories, data privacy, copyright, and licenses</li> </ul>"},{"location":"metadata/","title":"Metadata","text":"<p>Objectives\ud83d\udccd</p> <ul> <li>general Metadata (bibliographic/administrative Metadata)</li> <li>content and field specific Metadata </li> </ul>"},{"location":"metadata/#metadata-data-about-data","title":"Metadata = data about data","text":"<p>We all know and use metadata every day. See for yourself in this short video which kind of metadata you already know:</p> <p></p> <p>To get a sense of what Metadata for a research project should be you can simply ask yourself: \"What would someone unfamiliar with your data (and possibly your research) need in order to find, evaluate, understand, and reuse them?</p> <p>Task</p> <p>Let's imagine someone gives you a value of their data which only says \"Temperature 31.5\". What questions would you ask this researcher in order to find out what \"Temperature 31.5\" means and be able to reuse it or make a decision about if you can reuse it for your own research?</p> Possible questions <p>Temperature 31.5...</p> <ul> <li>of what?</li> <li>location?</li> <li>in what unit?</li> <li>is this value averaged?</li> <li>collected how?</li> <li>collected when?</li> <li>precision/accuracy?</li> <li>according to whom?</li> <li>has anyone checked the quality of this value?</li> </ul>"},{"location":"metadata/#general-metadata-for-a-research-project","title":"General metadata for a research project","text":"<p>Having data available is of no use if it cannot be understood. Without metadata to provide provenance and context, the data can't be used effectively. For example, a table of numbers is useless if no headings describe what the columns/rows contain. Therefore you should ensure that open datasets include consistent metadata, that is information about the data so that the data is fully described. This requires that information accompanying data is captured in documentation and metadata. </p> <p>Documentation provides context for your work.  It allows your collaborators, colleagues and future you to understand what has been done and why.</p> <p>Data documentation can be done on different levels.  All documentation accompanying data should be written in clear, plain language.  Documentation allows data users have sufficient information to understand the source, strengths, weaknesses, and analytical limitations of the data so that they can make informed decisions when using it. </p> <p>Metadata is information about the data, descriptors that facilitate cataloguing data and data discovery.  Often, metadata are intended for machine reading while documentation is mostly written for human reading.</p> <p>When data is submitted to a trusted data repository, the machine-readable metadata is generated by the repository.  If the data is not in a repository a text file with machine-readable metadata can be added as part of the documentation.</p> <ul> <li>The type of research and the nature of the data also influence what kind of documentation is necessary. </li> <li>The level of documentation and metadata will vary according to the project, and the range of people the data needs to be understood by.</li> <li>Examples of documentation may include items like data dictionaries (see here for a template) or codebooks, protocols, logbooks or lab journals, README files, research logs, analysis syntax, algorithms and code comments.  </li> <li>Variables should be defined and explained using data dictionaries or codebooks.</li> <li>Data should be stored in logical and hierarchical folder structures, with a README file used to describe the structure. The README file is helpful for others and will also help you find your data in the future. See the README template from Cornell for an example.</li> <li>It is best practice to use recognized community metadata standards to make it easier for datasets to be combined.</li> </ul> <p>Example README content:</p> <ul> <li>Short description of the study, motivation and background</li> <li>A description of the folder structure</li> <li>Time and location of data collection for the studies reported</li> <li>Software required to open or run any of the shared files</li> <li>Under which license(s) the files are shared: instructions for citing the project</li> <li>Information on the publication status of the studies</li> <li>Contact information for the authors: who the team members are and contact details of the project leads</li> <li>A list of all the shared files</li> </ul>"},{"location":"metadata/#tagging","title":"Tagging","text":"<p>Tags are keywords assigned to files, and a way to add metadata to a file to organise them more flexibly. While a file can only be in one folder at a time, it can have an unlimited number of tags. </p> <p>Some tips include:</p> <ul> <li>Use short tag names (one or two words)</li> <li>Be consistent with tags</li> <li>Not all file formats allow tags, and when files are transferred tags may be stripped</li> </ul> <p>See Tagging and Finding Your Files by MIT libraries for more information. </p> <p>Task</p> <p>Go to your OSF project and insert the general Metadata. Give your project some tags.</p> <p>Task</p> <p>Make a README.md file for your project and with the content described in the README example above. </p>"},{"location":"metadata/#community-standards-metadata","title":"Community Standards - Metadata","text":"<p>The use of community-defined standards for metadata is vital for reproducible research and allows for the comparison of heterogeneous data from multiple sources, domains and disciplines. Metadata standards are also discipline-specific. Not every discipline may use metadata standards, however. For the field of psychology and neuroscience, BIDS defines for the different data modalities which metadata has to be available in the dataset, how it is supposed to be named and in which folder it has to be placed. </p> <p>Again, for the field of psychology and neuroscience, BIDS defines for the different data modalities which metadata has to be available in the dataset, how it is supposed to be named and in which folder it has to be placed. </p> <p>There are also situations when researchers make use of more general metadata standards, for example when they use a generic archive to store their data they have to adhere to the metadata standards of the archive. In this case, a text file with discipline specific metadata can be added as part of the documentation. For this, you can also use the DataCite generator by the DataCite Metadata Working Group. <sup>1</sup> <sup>2</sup></p> <p>Task</p> <p>Go to the DataCite generator and generate a metadata file for your project.</p>"},{"location":"metadata/#optionalreadingfurther-materials","title":"optional/reading/further materials","text":"<ul> <li>Full BIDS documentation</li> <li>BIDS Starter Kit Tutorials</li> <li>BIDS Talks where the background and philosophy of BIDS are explained </li> <li>BIDS cookbook</li> <li>BIDS converters</li> <li>BIDS validator: checks if your dataset is in a valid BIDS format</li> <li>BIDS Apps</li> <li>You are very welcome to reach out to me if you want to start using BIDS. I know it's a lot and can be overwhelming in the beginning. I'm happy to help you get started!</li> </ul> <p>Info</p> <p>Most of the content was copied from The Turing Way Handbook under a CC-BY 4.0 licence.</p> <ol> <li> <p>DataCite Metadata Working Group. (2021). DataCite Metadata Schema Documentation for the Publication and Citation of Research Data and Other Research Outputs. Version 4.4. DataCite e.V. https://doi.org/10.14454/3w3z-sa82\u00a0\u21a9</p> </li> <li> <p>Bayer, Christiane, Frech, Andreas, Gabriel, Vanessa, K\u00fcmmet, Sonja, L\u00fccke, Stephan, Munke, Johannes, Putnings, Markus, Rohrwild, J\u00fcrgen, Schulz, Julian, Spenger, Martin, &amp; Weber, Tobias. (2022). DataCite Best Practice Guide (Version 2.0). Zenodo. https://doi.org/10.5281/zenodo.7040047\u00a0\u21a9</p> </li> </ol>"},{"location":"organization/","title":"Project and Data Organization","text":"<p>Objectives\ud83d\udccd</p> <ul> <li>folder structure</li> <li>file naming</li> <li>variable naming</li> <li>backups</li> </ul>"},{"location":"organization/#project-organization","title":"Project Organization","text":"<p>To organize your data, you should use a clear folder structure to ensure that you can find your files. For this there are already multiple existing templates.</p> The Turing Way_ project illustration by Scriberia. Used under a CC-BY 4.0 licence. DOI: [10.5281/zenodo.3332807](https://doi.org/10.5281/zenodo.3332807) <p>If you don't find any template that suits your needs (which I doubt...), make sure you follow these general suggestions on organization of folders: </p> <ul> <li>Make sure you have enough (sub)folders so that files can be stored in the right folder and are not scattered in folders where they do not belong, or stored in large quantities in a single folder.</li> <li>Use a clear folder structure. You can structure folders based on the person that has generated the data/folder, chronologically (month, year, sessions), per project (as done in the example below), or based on analysis method/equipment or data type.</li> <li>Avoid overlapping or vague folder names, and do not use personal data in folder/file names.</li> <li>All good structures contain at least the following elements:<ul> <li>A unique main folder for the project</li> <li>Some form of code</li> <li>Some form of data</li> <li>A readme document with any important information about the project for yourself or collaborators</li> </ul> </li> </ul>"},{"location":"organization/#project-organization-one-example","title":"Project Organization: One Example <sup>1</sup>","text":"<pre><code>.\n\u251c\u2500\u2500 LICENSE.md\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 code\n\u2502   \u251c\u2500\u2500 experiment\n\u2502   \u251c\u2500\u2500 analysis\n\u251c\u2500\u2500 data              \n\u2502   \u251c\u2500\u2500 source         &lt;- The original, immutable data dump. \n\u2502   \u251c\u2500\u2500 raw            &lt;- The final, canonical data sets for modeling.\n\u2502   \u251c\u2500\u2500 derivatives    &lt;- Data after being processed by analysis pipelines.\n\u2502   \u2514\u2500\u2500 temp           &lt;- Intermediate data that has been transformed. \n\u251c\u2500\u2500 docs               &lt;- Documentation notebook for users.\n\u2502   \u251c\u2500\u2500 manuscript     &lt;- Manuscript source, e.g., LaTeX, Markdown, etc.\n\u2502   \u251c\u2500\u2500 study-protocol &lt;- or `preregistration`\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 reports        &lt;- Other project reports and notebooks (e.g. Jupyter, .Rmd)\n\u2514\u2500\u2500 results\n    \u251c\u2500\u2500 figures        &lt;- Figures for the manuscript or reports \n \u00a0\u00a0 \u2514\u2500\u2500 output         &lt;- Other output for the manuscript or reports \n</code></pre> <p>It doesn't matter too much if you name the folder with processed data <code>processed</code> and have it under the upper folder <code>data</code> or if you have it one level up and name it instead <code>edited data</code> or what have you. The important thing is that you choose a structure and stick to it and best explain it to others by putting an explanation of the project folder structure in a README. What you should do though is checking out if there are community standards for folder structure in your research field. For example, in psychology and neuroscience we have a widely accepted and comprehensively documented standard called BIDS. Some data repositories now expect you to have the data in this format, otherwise they won't accept your data. So, just make sure that you follow recommendations and principles in your field of research (if there are any). </p> <p>How you organize your <code>data</code> folder depends on your kind of research data. For some it is better to have separate folders for each subject, e.g., if you collect multiple modality data from subjects. Then you can organize your <code>data</code> folder as such (inspired by BIDS for psychology and neurosciences):</p> <pre><code>\u251c\u2500\u2500 data              \n\u2502   \u251c\u2500\u2500 source         \n\u2502   \u251c\u2500\u2500 raw  \n\u2502   \u2502   \u251c\u2500\u2500 sub-01\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 beh\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 sub-01_beh.csv\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 eeg\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 sub-01_eeg.edf\n\u2502   \u2502   \u251c\u2500\u2500 sub-02\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 beh\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 sub-02_beh.csv\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 eeg\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 sub-02_eeg.edf\n\u2502   \u251c\u2500\u2500 derivatives    \n\u2502   \u2514\u2500\u2500 temp    \n</code></pre> <p>If you only have one data type, you can skip the <code>sub-0X</code> and <code>modality</code> folders, like so:</p> <pre><code>\u251c\u2500\u2500 data              \n\u2502   \u251c\u2500\u2500 source         \n\u2502   \u251c\u2500\u2500 raw  \n\u2502   \u2502   \u251c\u2500\u2500 sub-01_beh.csv\n\u2502   \u2502   \u251c\u2500\u2500 sub-02_beh.csv\n\u2502   \u251c\u2500\u2500 derivatives    \n\u2502   \u2514\u2500\u2500 temp    \n</code></pre> <p>Task</p> <p>Go through your project folder and re-organize it using the above mentioned recommendations. </p>"},{"location":"organization/#file-naming-conventions","title":"File Naming Conventions","text":"<p>Structure your file names and set up a template for this. For example, it may be advantageous to start naming your files with the date each file was generated. This will sort your files chronologically and create a unique identifier for each file. The utility of this process is apparent when you generate multiple files on the same day that may need to be versioned to avoid overwriting.</p> <p>Some other tips for file naming include:</p> <ul> <li>Use the date or date range of the experiment: <code>YYYY-MM-DD</code></li> <li>Use the file type</li> <li>Use the researcher's name/initials</li> <li>Use the version number of file (v001, v002) or language used in the document (ENG)</li> <li>Do not make file names too long (this can complicate file transfers)</li> <li>Avoid special characters (?!@*%{[&lt;&gt;) and spaces</li> <li>Avoid personal data in file names</li> </ul> <p>You can explain the file naming convention in a README file so that it will also become apparent to others what the file names mean.</p> <p>Jenny Bryan\u2019s \u2018naming things\u2019 presentation (also available as a 5 minute summary video on youtube) gives very concrete and intuitive recommendations and examples. Here's the main content of her talk for you: </p> Don't do this Do this instead myabstract.docx 2022-09-24_abstract-for-normconf.docx Jane\u2019s Filenames Use \u201cSpaces\u201d &amp; Punctuation ;).xlsx janes-filenames-are-getting-better.xlsx figure 1.png fig01_scatterplot-talk-length-vs-interest.png JW7d^(2sl@deletethisandyourcareerisoverWx2*.txt 1986-01-28_raw-data-from-challenger-o-rings.txt <p>Good file names are:</p> <ul> <li>machine readable</li> <li>human readable</li> <li>sorted in a useful way</li> </ul> <p>Machine readable</p> <p>Wikipedia: globbing = \"glob patterns specify sets of filenames with wildcard characters. For example, the Unix Bash shell command <code>mv *.txt textfiles/</code> moves all files with names ending in .txt from the current directory to the directory textfiles. Here, * is a wildcard and *.txt is a glob pattern. The wildcard * stands for \"any string of any length including empty, but excluding the path separator characters (/ in unix and  in windows)\". \"</p> <p>This means: use  <code>_</code> underscore to delimit fields, i.e. when you have multiple <code>.csv</code> files that contain data of one type of observations and need to be parsed to one <code>dataframe</code> in the end, use the observation name in the file name and delimit this name from the rest of the filename using an underscore. This way you can easily find it with the <code>ls</code> command and easily code the read-in for data analysis. At the same time, use  <code>-</code> hyphen to delimit words within fields.</p> <p>Example: If you have a list of files named like this <code>2022-09-24_Plasmid-Cellline-100-1MutantFraction_A01.csv</code>, <code>2022-06-26_Plasmid-Cellline-100-1MutantFraction_H02.csv</code>, <code>2022-06-26_Plasmid-Cellline-100-1MutantFraction_H03.csv</code> you can do multiple machine operations with it:</p> <ul> <li>you can find those files in your folder under all the other files by simply typing <code>ls *Plasmid*</code> in the terminal</li> <li>you can read in the different parts of the filename as headers for your dataframe by coding a delimiter rule, e.g., in <code>R</code> a code like: </li> </ul> <pre><code>separate_wider_delim(\n    filenames,\n    delim = regex(\"[_\\\\.]\"),\n    names = c(\"date\", \"assay\", \"well\", NA)\n)\n</code></pre> <p>leads to an output of:</p> date assay well 1 2022-09-24 Plasmid-Cellline-100-1MutantFraction A01 2 2022-06-26 Plasmid-Cellline-100-1MutantFraction H02 3 2022-06-26 Plasmid-Cellline-100-1MutantFraction H03 <p>Human readable</p> <p>Make sure that at least you yourself are able to decode from the filename what is in it. Try also to make it easy for others to guess what something is. </p> Don't do this Do this instead 01.md 01_marshal-data.md 01.R 01_marshal-data.R 02.md 02_pre-dea-filtering.md 02.R 02_pre-dea-filtering.R 03.md 03_dea-with-limma-voom.md 03.R 03_dea-with-limma-voom.R 04.md 04_explore-dea-results.md 04.R 04_explore-dea-results.R 9.md 90_limma-model-term-name-fiasco.md 90.R 90_limma-model-term-name-fiasco.R <p>Dates</p> <p>To be able to sort file in a chronological order it is always a good idea to include a date in the filename. For this you should respect <code>ISO 8601</code> which states that dates should be written in the <code>YYYY-MM-DD</code> format. Don't let the US convince you to use MM-DD-YYYY...they're really the only ones using this format.</p> <p>Sorted in a useful way</p> <ul> <li>plan for alphanumeric sorting</li> <li>put something numeric-ish first-ish</li> <li>use the ISO 8601 standard for dates</li> <li>left pad numbers with zeros. Otherwise the file starting with a <code>10</code> will be shown above the file starting with a <code>1</code> in the folder, which is confusing. </li> </ul>"},{"location":"organization/#variable-naming","title":"Variable naming","text":"<p>Basically, all of the above about file naming conventions applies equally to variable naming. Here are some conventions you should know about:</p> <ul> <li>CamelCase</li> <li>lowerCamelCase</li> <li>Underscore_Methods</li> <li>Mixed_Case_with_Underscores</li> <li>lowercase</li> </ul> <p>Task</p> <p>Go through your project files and rename files and variables using the above mentioned recommendations.</p> <p>Task</p> <p>Go to your OSF project and upload your newly organized folder. You can either upload the whole folder in your OSF project main page or use the components feature in OSF. </p> <p>Again, checking out if there are community standards in your research field for file and variable naming is a good idea. The above mentioned BIDS standard, for example, gives instructions on that, too. </p>"},{"location":"organization/#open-file-formats","title":"Open file formats","text":"<p>This one is short: use open file formats! Open file formats are the ones that can be opened by anyone on any machine without having to install additional software than the one a computer already comes with. Formats like .txt/.md for text or .csv/.tsv for tables can be opened by simple text editors. Formats from proprietary software (like Microsoft Office) cannot. </p>"},{"location":"organization/#optionalreadingfurther-materials","title":"optional/reading/further materials","text":""},{"location":"organization/#project-organisation-other-examples","title":"Project Organisation: Other Examples","text":"<ul> <li>This folder structure by Nikola Vukovic</li> <li>You can pull/download folder structures using GitHub: This template by Barbara Vreede, based on cookiecutter, follows recommended practices for scientific computing by Wilson et al. (2017).</li> <li>See this template by Chris Hartgerink for file organisation on the Open Science Framework.</li> <li>How to Organize Your Digital Files by Melanie Pinola.</li> <li>Project structure videos by Danielle Navarro (with slides).</li> </ul>"},{"location":"organization/#more-information-on-project-organisation","title":"More Information on Project Organisation","text":"<ul> <li>How to organise your data and code by Rene Bekkers. </li> </ul>"},{"location":"organization/#more-information-on-file-naming","title":"More Information on File naming","text":"<ul> <li>MIT's recommendations on File naming and folder hierarchy</li> <li>8 step guide on how to set up your file naming convention </li> </ul>"},{"location":"organization/#file-renaming-tools","title":"File renaming tools","text":"<p>If you want to change your file names you have the option to use bulk renaming tools. Be careful with these tools, because changes made with bulk renaming tools may be too rigorous if not carefully checked!</p> <p>Some bulk file renaming tools include:  - Bulk Rename Utility, WildRename, and Ant Renamer (for Windows) - Renamer (for MacOS) - PSRenamer (for MacOS, Windows, Unix, Linux)</p> <p>Info</p> <p>Most of the content was copied from The Turing Way Handbook under a CC-BY 4.0 licence.</p> <ol> <li> <p>adapted from this template by Barbara Vreede.\u00a0\u21a9</p> </li> </ol>"},{"location":"osf/","title":"Open Science Framework","text":"<p>Objectives\ud83d\udccd</p> <ul> <li>What OSF is and why it is useful</li> <li>First settings for an OSF project</li> </ul> <p>Here are the main points about the OSF, retrieved from their paper<sup>1</sup>, p.203-204:</p> <ul> <li>\"The Open Science Framework (OSF) is a tool that promotes open, centralized workflows by enabling capture of different aspects and products of the research lifecycle, including developing a research idea, designing a study, storing and analyzing collected data, and writing and publishing reports or papers.\" </li> <li>\"The core functionality of the OSF is its ability to create and develop projects.\"</li> <li>\"Each user, project, component, and file is given a unique, persistent uniform resource locator (URL) to enable sharing and promote attribution.\"</li> <li>\"Projects can also be assigned digital object identifiers (DOIs) and archival resource keys (ARKs) if they are made publicly available.\" </li> <li>\"The OSF provides built-in version control that records changes to project files and previous versions through OSF Storage.\" </li> <li>\"The OSF is intended to be collaborative, and users can easily add contributors to projects.\"</li> <li>\"While there are many features built into the OSF, the platform also allows third-party add-ons or integrations that strengthen the functionality and collaborative nature of the OSF.\" </li> <li>\"Registration is a major feature of the OSF and its efforts to preserve, provide access to, and promote transparency in research.\" </li> </ul> <p>For a very nice overview of how OSF supports you in your work during the whole Research Life Cycle, I recommend to watch this introduction video by OSF:</p> <p></p>"},{"location":"osf/#osf-project-first-settings","title":"OSF Project - First Settings","text":"<p>As you've read above and/or seen in the video, OSF supports you in the management of your project through its whole life cycle. During this school, we want to learn how to use the OSF services. The best way of learning how to use OSF is by doing. </p> <p>Task</p> <p>Go to OSF and create a new project for your own project and complete the first settings. </p> <p>Here's a short explanation what settings you can choose from in the beginning:</p> <p></p> <p>Extra note on \"components\": If your project contains data that you don't want to share openly, you should organize your project through the components feature on OSF as you can choose the visibility feature for every component differently. </p> <p>Over the course of this workshop we will fill this OSF project with all the input and output from your own project. </p> <ol> <li> <p>Foster, E. D., &amp; Deardorff, A. (2017). Open Science Framework (OSF). Journal of the Medical Library Association : JMLA, 105(2), 203\u2013206. https://doi.org/10.5195/jmla.2017.88\u00a0\u21a9</p> </li> </ol>"},{"location":"preregistration/","title":"Preregistration","text":"<p>Preregistration is nothing more than making your a-priori hypotheses about your data public before running analyses on this very data. It follows the scientific path of generating new hypotheses based on previous observations and then testing these hypotheses on a new dataset. In short: You predict how your data will look like without knowing how your data looks like yet. Nosek et al. (2018) refer to this as prediction (also known as confirmatory research). This is in contrast to generating new hypotheses after you already inspected your data, i.e., making your research more about why your data looks like as it does and thus applying data-contingent analyses. Nosek et al. (2018) call this postdiction (exploratory research). While there are obviously good reasons and the necessity for either, only with predictive research we are eventually able to reduce uncertainty.</p> <p>Preregistration isn\u2019t about eliminating postdiction. Postdictions happen continuously with every new study. Preregistration is simply about distinguishing prediction and postdiction in one study: results showing diagnostic value by being predictable vs. unforeseen results possibly building the ground for new predictions (or: what is likely vs. what is possible). Selling postdictive results as predictable fosters overconfidence in evidence that might not exist. In other words, preregistration ultimately defines the meaningfulness of reproducibility.</p> <p>Hint: if you already made a Research data management (RDM) plan the preregistration will be even easier and faster as some of the preregistration criteria overlap with specifications in a RDM plan.</p> <p>Task</p> <p>Go to the OSF. Explore some of the pre-registrations to get a sense of what will be asked. Then, go to our OSF project and do a pre-registration for your own project within this OSF project.</p>"},{"location":"rdmp/","title":"Research Data Management Plan","text":"<p>Objectives\ud83d\udccd</p> <ul> <li>What a RDMP is</li> <li>Why it is important</li> <li>RDMP components</li> <li>Research Data Management Organiser (RDMO) </li> </ul> <p>A Research Data Management Plan (RDMP) is a formal document that describes the handling of research data during and after the project. Many funding sources require such a RDMP when applying for funding. Irrespective of if you want to require funding or not, having a RDMP is best practice. It's basically a roadmap for you to manage your data efficiently and securely. It not only prevents data loss but will save you time later on. A RDMP is a living document, meaning that it can change over time as conducting research sometimes leads to revising the intended path and thus so should your RDMP. </p> Research Data Management Plan. The Turing Way project illustration by Scriberia. Reused under a CC-BY 4.0 license. Original version on Zenodo. http://doi.org/10.5281/zenodo.3695300."},{"location":"rdmp/#elements-of-a-dmp","title":"Elements of a DMP<sup>1</sup>","text":"Types of data - What is the source of your data?  - In what formats are your data? - Will your data be fixed, or will it change over time? - How much data will your project produce? Contextual details (metadata) How will you document and describe your data? Storage, backup, and security How and where will you store and secure your data? Provisions for protection/privacy What privacy and confidentiality issues must you address? Policies for re-use How may other researchers use your data? Access and sharing - How will you provide access to your data by other researchers? - How will others discover your data? Archiving and providing access What are your plans for preserving the data and providing long-term access? Roles and plan oversight Who will be responsible for aspects of data management throughout the project, and what resources are required for implementation? <p>Task</p> <p>Go to this Google Doc and download the RDMP file into your own project folder. Fill this file with all the information you can think of for your own project. </p>"},{"location":"rdmp/#research-data-management-plan-organizer-rdmo-or-dmp-tool","title":"Research Data Management Plan Organizer (RDMO) (or DMP Tool)","text":"<p>Universities have accommodated to the need of researchers to create a RDMP and organize their research. It is very likely that your University provides you with a tool that helps you to create a RDMP and saves it for you in a organized environment. Within that tool it is also possible to organize different RDMPs according to their working group or collaborative research consortia such as the SFB135. For the University Marburg, this tool is the RDMO. Such tools are often hosted by the university libraries, more specifically by the offices of the university library that are responsible for research data management. Just check it out for your university!</p> <ol> <li> <p>Descriptions retrieved from Longwood Research Data Management at Harvard Medical School.\u00a0\u21a9</p> </li> </ol>"},{"location":"reproducible-analysis/","title":"Reproducible analyses","text":"<p>Objectives\ud83d\udccd</p> <ul> <li>the jupyter ecosystem</li> <li>levels of computational environments</li> </ul>"},{"location":"reproducible-analysis/#jupyter-notebooks","title":"Jupyter notebooks","text":"<p>A Jupyter notebook is a shareable document that combines computer code, plain language descriptions, data, rich visualizations like 3D models, charts, graphs and figures, and interactive controls. A notebook, along with an editor (like JupyterLab), provides a fast interactive environment for prototyping and explaining code, exploring and visualizing data, and sharing ideas with others.</p> <p>Writing your analyses in a <code>jupyter notebook</code> not only brings benefits for your own work but can ensure the reproducibility of your results. You can share your <code>jupyter notebook</code> together with your conventional paper and thus other researchers or the reviewer of your paper can directly track and reproduce your analysis. This gives high credibility of your work. Besides that, it's just so awesome with it's many features, the big community and support! </p> Example of a jupyter notebook. <p>Some universities offer a so-called JupyterHub which is basically an environment to maintain a <code>JupyterLab</code>. JupyterLab enables you to work with documents and activities such as Jupyter notebooks, text editors, terminals, and custom components in a flexible, integrated, and extensible manner. For a demonstration of JupyterLab and its features, you can view this video:</p> <p></p>"},{"location":"reproducible-analysis/#computational-environments","title":"Computational Environments","text":"<p>Info</p> <p>Most of the content was copied from Peer Herholz's \"There and back again: a short introduction to virtualization technologies\" and from Docker vs. python virtualization vs. virtual machines by Dr. Stephen Odaibo</p> <p>The problem statement</p> <p>Imagine you want to conduct an analysis of some demographic data, including obtaining &amp; reading data, filtering &amp; descriptive analyses of data, inferential statistics and visualization. A colleague has a python script that does all of these things ready to go and shares it with you. Everything is ok\u2026.</p> <p>The script doesn\u2019t run? The script leads to different results?  </p> ...but it worked on my machine?! <p>What went wrong?</p> <ul> <li>each and every single project in a lab depends on complex software environments:<ul> <li>operating system</li> <li>Drivers</li> <li>Software dependencies: Python, R, MATLAB + libraries</li> </ul> </li> </ul> <p>This leads to statements like: </p> <ul> <li>\"The computer I used was shut down a year ago, I can\u2019t rerun the analyzes from my publication\u2026\"</li> <li>\"The analyzes were run by my student, I have no idea where and how...\" </li> </ul> <p>Virtualization technologies aim to isolate the computing environment by providing a mechanism to encapsulate environments in a self-contained unit that can run anywhere. Thus, with virtualization techniques it is possible to reconstruct and share computing environments.</p> <p>Virtualization technologies have 3 main types:</p> <p></p> python virtualization containers virtual machines venv Docker Virtualbox conda Singularity VMware <p>Python virtualization</p> <p>Python virtual environments are a mechanism to prevent incompatibility clashes and other forms of conflict that arise from 3<sup>rd</sup> party python libraries share space to an extent. For instance, an update of tensorflow from 1.13 to 2.0 may result in breakage of any applications that relied particularly on tensorflow 1.13. </p> <p>To avoid this problem one would like to configure environments that have specific signatures as pertains to 3<sup>rd</sup> party python packages. For instance, one virtual environment could be out TF2.0/Keras 2.2.5/Python 2.7.14 environment, while another is our TF2.0/Keras 2.0/Python 3.6.8 environment, and yet another our TF1.10-gpu/Keras 2.3.0/Python3.6.0 environment. This setup facilitates sandboxing and encourages experimentation by greatly decreasing the risk that we will break anything. </p> <p>Python virtual environments...</p> <ul> <li>keep the dependencies required by different projects in separate places</li> <li>allows you to work with specific version of libraries or Python itself without affecting other Python projects</li> <li>Applications<ul> <li>venv: an environment manager for Python 3.4 and up, usually preinstalled</li> <li>conda: an environment manager and package manager (for python and beyond)</li> </ul> </li> </ul> <p>Containers</p> <p>A container wraps an application\u2019s software into an invisible box with everything the application needs to run. That includes the operating system, application code, runtime, system tools, system libraries, and etc. All the operating system level architecture is being shared across containers. One of those container systems is Docker. Docker containers have fully prescribed dependencies with which they can be created. These dependencies as well as the instruction on how specifically to create the container are stored in the container\u2019s image. The image of a container is portable and can be registered on one of a number of registries/repositories/hubs. Once there, anyone can \u201cpull it\u201d so long as they know its unique name.</p> <p>Unlike a VM which provides hardware virtualization, a container provides operating-system-level virtualization by abstracting the \u201cuser space\u201d. </p>"},{"location":"reproducible-analysis/#optionalreadingfurther-materials","title":"optional/reading/further materials","text":"<ul> <li>Understanding Conda and Pip</li> <li>A beginner friendly intro to VMs and Docker</li> <li>Intro to Docker from Neurohackweek</li> <li>Understanding Images</li> <li>Singularity examples</li> <li>one day docker workshop</li> <li>Docker vs. python virtualization vs. virtual machines</li> <li>Brainhack School Module: Containers</li> <li>Brainhack School Module: Intro to Python</li> <li>Brainhack School Module: Data Visualization with Python</li> <li>Brainhack School Module: Writing Scripts in Python</li> </ul>"},{"location":"storage_sharing/","title":"Data Storage and Sharing","text":"<p>Objectives\ud83d\udccd</p> <ul> <li>different data repositories</li> <li>what to consider when sharing data</li> </ul>"},{"location":"storage_sharing/#where-to-store-data","title":"Where to Store Data","text":"<ul> <li>Most institutions will provide a network drive that you can use to store data.</li> <li>Portable storage media such as memory sticks (USB sticks) are more risky and vulnerable to loss and damage.</li> <li>Cloud storage provides a convenient way to store, backup and retrieve data. You should check terms of use before using them for your research data.</li> </ul> <p>Especially if you are handling personal or sensitive data, you need to ensure the cloud option is compliant with any data protection rules the data is bound by. To add an extra layer of security, you should encrypt devices and files where needed.</p> <p>Your institution might provide local storage solutions and policies or guidelines restricting what you can use. Thus, we recommend you familiarise yourself with your local policies and recommendations. </p>"},{"location":"storage_sharing/#backups","title":"Backups","text":"<p>To avoid losing your data, you should follow good backup practices.</p> <ul> <li>You should have 2 or 3 copies of your files, stored on</li> <li>at least 2 different storage media,</li> <li>in different locations.</li> </ul> <p>Backups are ideally done automatically and should take into consideration your institute's guidelines. The more important the data and the more often the datasets change, the more frequently you should back them up. If your files take up a large amount of space and backing up all of them proves to be challenging or expensive, you may want to create a set of criteria for when you back up the data. This can be part of your RDMP.</p> <p>Watch this video on Safe data storage and backup from the TU Delft Open Science MOOC.</p>"},{"location":"storage_sharing/#data-repositories","title":"Data Repositories","text":"<p>When you are ready to release the data to the wider community, you can also search for the appropriate databases and repositories in FAIRsharing, according to your data type, and type of access to the data.</p> <p>A repository is a place where digital objects can be stored and shared with others (see also this repository definition).</p> <p>Data repositories provide access to academic outputs that are reliably accessible to any web user (see the OpenDOAR inclusion criteria).  Repositories must earn the trust of the communities they intend to serve and demonstrate that they are reliable and capable of appropriately managing the data they hold (Lin et al. (2020)).</p> <p>Long-term archiving repositories are designed for secure and permanent storage of data, ensuring data preservation over extended periods. This differs from platforms like GitHub and GitLab which primarily serve as collaborative development tools, facilitating version control and project management in a more dynamic and transient environment. Platforms such as GitHub and GitLab do not assign persistent identifiers to repositories, and their preservation policies are more flexible compared to those of data repositories. </p>"},{"location":"storage_sharing/#repositories-and-fair","title":"Repositories and FAIR","text":"<p>Selecting an appropriate repository for your research outputs has many benefits:</p> <ul> <li> <p>It helps make your Research Objects more FAIR. This is achieved through:</p> <ul> <li>Repositories assign a Persistent Identifier to your Research Objects, which makes them findable and citable. The most commonly used persistent identifiers for research objects is the Digital Object Identifier, usually abbreviated to DOI.</li> <li>Repositories use metadata standards in describing your Research Object, which ensures that other people can find it using search engines.</li> <li>Repositories add a licence to the Research Objects. A license describes to potential reusers of your work what they are allowed to do with it. </li> <li>Repositories provide documentation for Research Objects. This can be in the form of READMEs and/or wikis that provide a description of your project and why it might be relevant to people.</li> <li>Encouraging widely-used file formats. Many repositories have restrictions on the file formats used to ensure the sustainability of Research Objects. Some file formats (especially proprietary ones with a limited user base) can become deprecated.</li> </ul> </li> <li> <p>It allows to determine the levels of access to Research Objects. There are good reasons to not to make all Research Objects completely open. However, it's still worthwhile to at least open the metadata and provide an option for people to obtain access to the actual Research Objects if they have certain credentials or if they have been given explicit access. That way, your work will still be FAIR (because the metadata are findable and there is an access procedure in place), as well as and secure (because you can control who has access). Restricting access and storing data on European servers can help to manage sensitive data.</p> </li> </ul>"},{"location":"storage_sharing/#selecting-an-appropriate-repository","title":"Selecting an appropriate repository","text":"<p>Data should be submitted to domain or discipline specific, community recognized, repository where possible. A general purpose repository can be used when there are no suitable discipline specific repositories. Discipline specific data repositories are likely to have more functionalities for the type of data that you would like to share, as well as community standards that you can adhere to to make the data more FAIR. </p> <p>The choice of repository can depend on multiple factors:</p> <ul> <li>Your discipline</li> <li>Type of digital output</li> <li>File size</li> <li>Policies/requirements from institutions, national policies, funding agencies</li> <li>Access restrictions</li> </ul> <p>You can search for relevant repositories on re3data and FAIRsharing. However, a search will likely result in a long list of repositories, which you will need to narrow down. The following questions may help you with that:</p> <ul> <li>Is the data repository discipline-specific and community-recognized? Does it use the recognized standards in my discipline?</li> <li>Is the data repository known by the research community?</li> <li>Are others using the data repository to share their data?</li> <li>Has a data repository been specified by my funder/publisher/institution?</li> <li>What are the file size requirements and limitations?</li> <li>What are the costs for data sharing?</li> <li>What data formats are allowed? Will it take the data that you want to share?</li> <li>Does it provide a persistent identifier, for example a Digital Object Identifier (DOI)?</li> <li>Does it provide the right type of access control that suits the sharing conditions of the data? (restricted access/embargo's)</li> <li>Is there support available on how to curate the data/metadata?</li> </ul> <p>See the ARDC's Guide to choosing a data repository or the DCC checklist for evaluating data repositories for more information. </p> <p>Your institution might also provide you with a data repository. </p> General repositories <p>If your disicpline does not have a disciplinary specific repository you can make use of several general repositories.  Below follows a (non-exhaustive) list of these different types of repositories: </p>"},{"location":"storage_sharing/#general-purpose-repositories","title":"General purpose repositories","text":"<ul> <li>Zenodo</li> <li>Figshare</li> </ul>"},{"location":"storage_sharing/#project-repositories","title":"Project repositories","text":"<ul> <li>Open Science Framework (OSF)</li> <li>Research Equals</li> <li>Octopus</li> <li>CRAN for R-Packages</li> </ul>"},{"location":"storage_sharing/#generic-data-repositories","title":"Generic data repositories","text":"<ul> <li>Dryad</li> <li>Dataverse</li> <li>4TU.ResearchData</li> <li>UK Data Service</li> </ul>"},{"location":"storage_sharing/#data-sharing","title":"Data Sharing","text":""},{"location":"storage_sharing/#motivations-for-sharing-data","title":"Motivations For Sharing Data","text":"<p>There are many reasons to share your research data publicly.</p> <ol> <li>To allow the possibility to fully reproduce a scientific study.</li> <li>To prevent duplicate efforts and speed up scientific progress. Large amounts of research funds and careers of researchers can be wasted by only sharing a small part of research in the form of publications.</li> <li>To facilitate collaboration and increase the impact and quality of scientific research.</li> <li>To make results of research openly available as a public good, since research is often publicly funded.</li> </ol> Steps To Share Your Data <p>Step 1: Select what data you want to share</p> <p>Not all data can be made openly available, due to ethical and commercial concerns, and you may decide that some of your intermediate data is too large to share. As such, you first need to decide which data you need to share for others to be able to reproduce your research.</p> <p>Step 2: Choose a data repository or other sharing platform</p> <p>Data should be shared in a formal, open, and indexed data repository where possible so that it will be accessible in the long run. Suitable data repositories by subject, content type or location can be found at Re3data.org, and in FAIRsharing where you can also see which standards (metadata and identifier) the repositories implement and which journal/publisher recommend them. If possible use a repository that assigns a DOI, a digital object identifier, to make it easier for others to cite your data. </p> <p>A few public data repositories are Zenodo, Figshare, Harvard Dataverse, 4TU.ResearchData, and Dryad.  See the NIH list of Generalist Repositories for more data repositories.</p> <p>Step 3: Choose a licence and link to your paper and code</p> <p>So that others know what they can do with your data, you need to apply a licence to your data. The most commonly used licences are Creative Commons, Open Government Licence, or an Open Data Commons Attribution License. To get maximum value from data sharing, make sure that your paper and code both link to your data, and vice versa, to allow others understand your project better.</p> <p>Step 4: Upload your data and documentation</p> <p>In line with the FAIR-principles, upload the data in open formats as much as possible and include sufficient documentation and metadata so that someone else can understand your data. It is also essential to think about the file formats in which the information is provided. Data should be presented in structured and standardized formats to support interoperability, traceability, and effective reuse. In many cases, this will include providing data in multiple, standardized formats, so that it can be processed by computers and used by people.</p>"},{"location":"storage_sharing/#data-availability-statement","title":"Data Availability Statement","text":"<p>Once you made your data available, it is important to ensure that people can find it when they read the associated article. You should cite your dataset directly in the paper in places where it is relevant, and include a citation in your reference list, as well as include a Data Availability Statement at the end of the paper (similar to the acknowledgement section).</p>"},{"location":"storage_sharing/#privacy-and-data-protection","title":"Privacy And Data Protection","text":"<p>Many fields of research involve working with sensitive personal data, with medical research being the most obvious example. There are a number of strategies that you can adopt to safeguard the privacy of your research subjects:</p> <p>1. Data minimisation</p> <ul> <li>If personal information isn't needed, don't collect it.</li> <li>Periodically review whether you are retaining unnecessary identifying information.</li> <li>When identifying information is no longer needed, safely remove, delete or destroy it.</li> </ul> <p>2. Data retention limits</p> <ul> <li>Decide how long you will retain identifiable data before removing direct identifiers, applying more complex anonymisation techniques, or deleting the data altogether.</li> <li>When deleting sensitive data you need to be aware that standard methods for deleting files (for example moving files to the recycle bin and emptying it) are not secure. These deleted files may be recovered.  Use software like BleachBit (Linux, Windows), BC Wipe, DeleteOnClick and Eraser (Windows) or Permanent Eraser or 'secure empty trash' (Mac) to safely delete the data. An alternative is the physical destruction of the storage media. Degaussing disturbs the magnetic alignment of magnetic storage media (such as hard drives and tapes) and may render those unusable.  If you encrypted the data (see point 4 below), you can also delete the encryption key.</li> </ul> <p>3. Secure data transfer</p> <ul> <li>Before deciding to transfer personal data, you should consider whether the transfer of identifiable data is necessary. For example, can data be de-identified or anonymised? </li> <li>If data cannot be made unidentifiable then you must ensure you have authority to transfer the personal data, and that there are appropriate safeguards in place to protect the data before, during and after transit.</li> <li>Keeping data in one place is safer than transfering it elsewhere.  Consider whether it is possible to provide access to the data, instead of transferring them outside of your institution.</li> <li>Often your university or institute will provide solutions for secure file transfer.  Contact you research data, privacy or IT support team for guidance. </li> </ul> <p>4. Encryption </p> <ul> <li>Encryption provides protection by ensuring that only someone with the relevant encryption key (or password) will be able to access the contents.<ul> <li>Protect on disk level: Bitlocker for Windows, FileVault for MacOS</li> <li>Protect on \u201ccontainer\u201d level (a folder containing multiple files):  Veracrypt (or Archive for MacOS)</li> <li>Portable storage: Bitlocker</li> <li>File level / Exchange information:</li> <li>Simple method: use 7zip, and pack with a password</li> <li>More complicated to setup: use PGP tooling (can also be used to securely send email)</li> <li>See the Ghent University Encryption for Researchers manual for more details and step-by-step guides</li> </ul> </li> </ul> <p>5. Access permissions</p> <ul> <li>Control who has access to which parts of the data, and which type of permissions they have, such as \"read\" vs. \"write\" access.</li> <li>Deny access to sensitive data if that access is no longer needed.</li> <li>Password protection.</li> </ul> <p>6. Anonymization</p> <p>Anonymization is a process by which identifying information in a dataset is removed.  It is used primarily to allow data to be shared or published without revealing the confidential information it contains.</p> <ul> <li>Where possible, direct identifiers (such as names, addresses, telephone numbers and account numbers) should be removed as soon as the identifying information is no longer needed.  You can delete the data or replace it with pseudonyms.  For qualitative data you should replace or generalise identifying characteristics when transcribing interviews.</li> <li>De-identified data that can be re-identified using a linkage file (for example, information linking data subjects to identifiable individuals) is known as pseudonymized data. NOTE: In this instance, the linkage file should be encrypted and stored securely and separately from the de-identified research data. Identification of individuals in pseudonymized or de-identified data may still be possible using combinations of indirect identifiers (such as age, education, employment, geographic area and medical conditions). Further, data and outputs containing small cell counts may be potentially disclosive, particularly where samples are drawn from small populations or include cases with extreme values or relatively rare characteristics.</li> </ul> <p>For more information about anonymization:</p> <ul> <li>Watch this webinar by Enrico Glerean </li> <li>Watch a presentation on Amnesia \u2013 Data Anonymisation Made Easy or a webinar on Amnesia - a tool to make anonymisation easy</li> <li>Or read an explanation by the Finnish social science data archive</li> <li>Anonymisation step-by-step</li> </ul> Citing Research Objects <p>You should cite research objects directly in the paper in places where it is relevant. This is a commonly practised way of citing publications and is valid for citing other research components like data and software. A citation includes the following information:</p> <ul> <li>Author</li> <li>Title</li> <li>Year of publication</li> <li>Publisher (for data, this is often the data repository where it is housed)</li> <li>Version (if indicated)</li> <li>Access information (a URL or DOI)</li> </ul> <p>A citation style is a specific arrangement, order and formatting of information necessary for a citation. For instance, the MLA style was developed by Modern Language Association (originally used in the humanities) and the APA style was developed by American Psychological Association (originally used in psychology and the social sciences).</p> <ul> <li>MLA citation style uses the following format: <code>Author. \"Title of the Source.\" Title of the Container, Other contributors, Version, Number, Publisher, Publication date, Location.</code></li> <li>APA citation style uses the following format: <code>Author. (Year). Title of data set (Version number). [Retrieved from] ***OR*** [DOI]</code></li> </ul> <p>See Scribbr Citation Styles Guide. See also FORCE11 resource.</p> Citing Data <p>When sharing a dataset, use the assigned DOI (from the data repository) and add this to your data availability statement at the end of the paper (similar to the acknowledgement section). It is important to also cite your dataset in the references themselves, as only the citations in the reference section will contribute to citation counts. Data citation is important because it facilitates access, transparency and potentially reproducibility, reuse, and credit for researchers. It also provides recognition and visibility for the repositories that share data.</p> <p>You can find examples of these statements in the publishers' (research data) author policies.</p> <p>Data availability statement examples:</p> <p>Using the Digital Object Identifier (DOI): \u201cThe data that support the findings of this study are openly available in [repository name] at http://doi.org/[doi].\u201d</p> <p>If no DOI is issued: - \u201cThe data that support the findings of this study are openly available in [repository name] at [URL], reference number [reference number].\u201d</p> <p>When there is an embargo period you can reserve your DOI and still include a reference to the dataset in your paper: - \u201cThe data that support the findings will be available in [repository name] at [URL / DOI] following a [6 month] embargo from the date of publication to allow for the commercialisation of research findings.\u201d</p> <p>When data cannot be made available: - \u201cRestrictions apply to the data that support the findings of this study. [Explain nature of restrictions, for example, if the data contains information that could compromise the privacy of research participants] Data are available upon reasonable request by contacting [name and contact details] and with permission of [third party name].\u201d -  \u201cThe data that support the findings of this study are available upon request. Access conditions and procedures can be found at [URL to restricted access repository such as EASY.]\u201d</p> <p>When code is shared: - Data and code to reproduce the results shown in the paper can be obtained from The Turing Way (2023) at Zenodo (https://zenodo.org/doi/10.5281/zenodo.3233853) and GitHub (https://github.com/the-turing-way/the-turing-way). We used R version 4.2.2 (use citation() to check the suggested citation) and the following R packages: ggplot2 (Wickham 2016), another example (and citation added to the references!). </p> <p>More Data Availability Statement examples:</p> <p>You can find more examples on the Manchester's Data Access Statements page, the Cambridge Data Availability Statement examples, the AMS Data Availability Statement examples, or Nature's Tips for writing a dazzling Data Availability Statement.</p> Citing Software <p>A software citation has a lot of the same elements as a data citation, described above, and are described in more detail in the Software Citation Principles. When using others software, it is vital to cite and attribute it properly. See also How to Cite R and R Packages for more information.</p> GitHubGitLab <p>To make your code citable, you can use the integration between Zenodo and GitHub.</p> <ul> <li>Create a file to tell people how to cite your software. Use this handy guide to format the file.</li> <li>Link your GitHub account with a Zenodo account. This guide explains how.</li> <li> <p>You can tell Zenodo what information or metadata you want to include with your software by converting your <code>CITATION.cff</code> file to <code>zenodo.json</code>.</p> <pre><code>pip install cffconvert\ncffconvert --validate\ncffconvert --format zenodo --outfile .zenodo.json\n</code></pre> </li> <li> <p>Add <code>.zenodo.json</code> to your repository.</p> </li> <li>On Zenodo, flip the switch to the 'on' position for the GitHub repository you want to release.</li> <li>On GitHub, click the Create a new release button. Zenodo should automatically be notified and should make a snapshot copy of the current state of your repository (just one branch, without any history), and should also assign a persistent identifier (DOI) to that snapshot.</li> <li>Use the DOI in any citations of your software and tell any collaborators and users to do the same!</li> </ul> <p>To make your code citable, through an automated publication of your Gitlab repository to Zenodo:</p> <ul> <li>Create a file to tell people how to cite your software. Use this handy guide to format the file.</li> <li> <p>Convert your <code>CITATION.cff</code> file to <code>.zenodo.json</code>. This file tells Zenodo what information or metadata you want to include with your software.</p> <pre><code>pip install cffconvert\ncffconvert --validate\ncffconvert --format zenodo --outfile .zenodo.json \n</code></pre> </li> <li> <p>Add <code>.zenodo.json</code> to your repository.</p> </li> <li>Use the gitlab2zenodo package to publish a snapshot of your repository to your Zenodo instance. By following the installation and setup instructions of this package, you will get a workflow on your CI options that will take care of the publication to Zenodo.</li> <li>Use the DOI in any citations of your software and tell any collaborators and users to do the same!</li> </ul> <p>If you don't have a Zenodo record for your software yet when you attempt to publish it for the first time, you may encounter an error due to the undefined <code>ID</code>. To address this issue, we recommend manually creating a record on Zenodo and updating the value of the CI variable <code>zenodo_record</code>. Detailed instructions for this process can be found in the gitlab2zenodo installation and setup instruction.</p>"},{"location":"storage_sharing/#additional-resources-on-data-sharing","title":"Additional resources on data sharing","text":"<ul> <li>'How can you make research data accessible?': a blog that contains five steps to make your data more accessible</li> <li>The European Commission's data guidelines</li> <li>Videos on Data sharing and reuse &amp; Data Preservation and Archiving from the TU Delft Open Science MOOC.</li> <li>Webinar: Why share your data?</li> <li>Webinar: Publishing and citing data in practice by Jez Cope</li> <li>Coursera Videos from Research Data Management and Sharing on the Benefits of Sharing, Why Archive Data?, and Why is Archiving Data Important?</li> <li>Blog: Ask not what you can do for open data; ask what open data can do for you</li> <li>Forschungsdaten.info: Datenschutzrecht (Data protection law) </li> <li>Brainhack School Module: Open Data</li> </ul> <p>Info</p> <p>Most of the content was copied from The Turing Way Handbook under a CC-BY 4.0 licence.</p>"},{"location":"version-control/","title":"Version Control and Provenance Tracking","text":"<p>Objectives\ud83d\udccd</p> <ul> <li>different version control systems and levels</li> <li>motivation for using version control</li> <li>version control with Git</li> <li>versioning data</li> </ul>"},{"location":"version-control/#summary","title":"Summary","text":"<p>No matter how your group is organized, the work of many contributors needs to be managed into a single set of shared working documents. Management of changes or revisions to any types of information made in a file or project is called versioning.</p> <p>In particular, reproducibility requires the provision of the code and the data that was used to produce a figure. In practice, data and code are modified regularly and one needs to record what was changed when, in order to provide provenance information.  As we will see in this chapter, version control has a lot of other advantages, which explains why most data science project are hosted on Git platforms.</p> <p>Version control is an approach to record changes made in a file or set of files over time so that you and your collaborators can track their history, review any changes, and revert or go back to earlier versions.  Management of changes or revisions to any types of information made in a file or project is called versioning. For example, when writing a paper with multiple collaborators, version control can help track what changed, who changed them, and what updates were made.</p> _The Turing Way_ project illustration by Scriberia. Used under a CC-BY 4.0 licence. DOI: [10.5281/zenodo.3332807](https://doi.org/10.5281/zenodo.3332807)"},{"location":"version-control/#version-control-systems","title":"Version control systems","text":"<p>Different version control systems can be used through a program with a graphical user interface, web browser-based applications, or command-line tools.Tools such as Google Drive and Dropbox offer platforms to update files and share them with others in real-time, collaboratively.</p> <p>More sophisticated version control system exists within tools like Google docs or HackMD.These allow collaborators to update files while storing each version in its version history (we will discuss this in detail).</p> <p>Advanced version control systems (VCS) such as Git, Mercurial, and SVN provide much more powerful tools. Accessing the version history and keeping control over the main version of your files are particular feature of these advanced tools.</p> <p>Versioning practices mainly come from managing changes in the code repositories. However, in reality, you can use version control for nearly any type of file on a computer.</p>"},{"location":"version-control/#motivation","title":"Motivation","text":"<p>In terms of reproducibility, version control is promordial in order to follow provenance information. Because data and analysis code do evolve over time, it can become very difficult or even impossible to know what version of the code and what version of the data was used to produce a particular figure. This provenance information is enabled and facilitated when both the data, the code and the figure files are under versioning.</p> <p>In addition, version control creates version history to help us understand what changes were made, or why a specific analysis was run, even weeks or months later. With the help of comments and commit messages in Git, for instance, each version can explain what changes it contains compared to the previous versions. This is helpful when we share our analysis (not only data), and make it auditable or reproducible - which is good scientific practice.</p> <p>A version control system neatly hide older versions of the data.  So your working directory is not cluttered by the debris of previous versions, while they remain accessible, in case you need them. Similarly, with version control, there is no need to leave unused chunks of code should you ever need to come back to an old version again.</p> <p>Finally, version control is invaluable for collaborative projects where different people work on the same data or code simultaneously and build on each other's work. Using a version control system, changes made by different people can be tracked and often automatically combined, saving a great deal of painstaking manual efforts. Using version control for your research project means that your work is more transparent. Because all your actions are recorded, your studies are easier to reproduce and build upon. Moreover, version control hosting services such as GitHub, GitLab and others provide a way to communicate and collaborate in a more structured way, such as in <code>pull requests</code>, <code>code reviews</code>, and <code>issues</code>.</p>"},{"location":"version-control/#potential","title":"Potential","text":"<p>Here is a non-exhaustive list of features that a Git/GitHub workflow bring to data science projects, and that would be useful for research projects:</p> <ul> <li>Backup data by pushing the data to a Git platform, toward a public or private repository.</li> <li>Easily use different computers to work on the same project (with yourself of with collaborators).</li> <li>Keep track of contributions.</li> <li>Facilitate the use of folder templates to help with files organization.</li> <li>Use Git platforms tools for project management.</li> <li>Use Git platforms for outreach, even when the repository is private (using the Wiki).</li> <li>Create an associated website under the same organization on the Git platform.</li> </ul> <p>I encourage you to use a Git platform that is provided as an open infrastructure. In many university, you will have access to a GitLab platform (which works very similarly to GitHub). </p>"},{"location":"version-control/#limitations-of-git","title":"Limitations of Git","text":"<p>Git does not work well when there are a lot of data, or when the data are large. When you expect the project to get large, one needs to set a different tooling to avoid creating unpractical repositories. Some of these tools makes it more difficult to access or see you files, so it is important to plan in advance what tool will best suits your need.</p> <p>Briefly, in order to use Git when there are lots or large files, one needs to split the data in different repositories, and have these repositories use the git-annex technology.</p>"},{"location":"version-control/#versioning-data","title":"Versioning Data","text":"<p>Git is made for small files and stems from software development where most of the files are code files. If you have many or large (and binary) files (such as huge data tables, images etc.), you will need to use the Git submodules and git-annex technologies. Version controlling the components of evolving projects could help to make work more organised, efficient, collaborative, and reproducible. Many scientific projects, however, do not only contain code, manuscripts, or other small-sized files, but contain larger files such as large datasets, analysis results, or binary files (presentations, manuscripts, pdfs) which can change or be updated in a project just like other small sized text components. In this chapter, we discuss why and how to do data versioning, especially why Git is not well suited for data versioning and what we can be done about it.</p>"},{"location":"version-control/#importance-of-version-controlling-data","title":"Importance of Version Controlling Data","text":"<p>We should not hold the notion that the data used for analysis is static; once it is acquired, it does not change and serves as input for a given analysis and the backbone of our scientific results. The reality is that data is only rarely invariant. For example, throughout a scientific project, datasets can be extended with new data, adapted to new naming schemes, reorganized into different file hierarchies, updated with new data points or modified to fix any errors. Sometimes you might also want to experiment off different versions of the same dataset.</p> <p>Such dynamic processes are excellent and beneficial for science as they ensure that data is usable and up-to-date, but they can be confusing if they are not adequately documented. If a dataset that is the basis for computing a scientific result changes without version control, reproducibility can be threatened: results may become invalid, or scripts that are based on file names that change between versions can break. Especially if original data gets replaced with new data without version control in place, the original results of the analysis may not be reproduced. Therefore, version controlling data and other large files in a similar way to version controlling code or manuscripts can help ensure the reproducibility of a project and capture the provenance of results; that is \"the precise subset and version of data a set of result originates from\". Together with all other components of a research project, data identified in precise versions is part of the research outcome. The reproducibility aspect of a scientific project can improve a lot if we can track the subset or version of data a certain analysis or result is based upon.</p> Provenance on which data in which version was underlying which computation is crucial for reproducibility. The Turing Way project illustration by Scriberia. Used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807."},{"location":"version-control/#challenges-in-version-controlling-data","title":"Challenges in Version Controlling Data","text":"<p>As we described earlier, there are limitations to Git. If others try to clone your repository or fetch/pull to update it locally, it will take longer to do this if it contains larger files that have been versioned and modified.</p> <p>Accordingly, repository hosting services usually impose maximum file sizes on users. For example, if a single file in your repository exceeds 100MB, you will not be able to push this file to a GitHub repository. Furthermore, if a large file was accidentally added to a repository, removing the file from the repository can be tedious, as this file needs to be purged.</p>"},{"location":"version-control/#tools-for-version-controlling-data","title":"Tools for Version Controlling Data","text":"<p>Several tools are available to handle version controlling and sharing large files. Most of them integrate very well with Git and extend a repository's capabilities to version control large files. With these tools, large data can be added to a repository, version controlled, reverted to previous states, or updated and modified collaboratively, and even shared via GitHub as small-sized files. Some of these tools include:</p>"},{"location":"version-control/#dvc","title":"DVC","text":"<p>DVC (open-source Version Control System for Machine Learning Projects) https://dvc.org/. DVC guarantees reproducibility by consistently maintaining a combination of input data, configuration, and the code that was initially used to run an experiment.</p>"},{"location":"version-control/#git-lfs","title":"Git LFS","text":"<p>Git LFS comes with a command-line extension to Git and allows you to treat files of any size alike, using standard Git commands. A major shortcoming, however, is that Git LFS is a centralised solution. Large files are not distributed but stored on a remote server. This usually requires setting up your server or paying for a service - which can make it very inaccessible.</p>"},{"location":"version-control/#git-annex","title":"<code>git-annex</code>","text":"<p>The <code>git-annex</code> tool is a distributed system that can manage and share large files independent from a central service or server. <code>git-annex</code> manages all file content in a separate directory in the repository (<code>.git/annex/objects</code>, the so-called annex) and only places file names with some metadata into version control by Git. When a Git repository with an annex is pushed to a web-hosting service such as GitHub, the contents stored in the annex are not uploaded. Instead, they can be pushed to a storage system (such as a web server, but also third party services such as Dropbox, Google Drive, Amazon S3, box.com, and many more). If a repository with an annex is cloned, the clone will not contain the contents of all annexed files by default, but display only file names. This makes the repository small, even if it tracks hundreds of gigabytes of data, and cloning fast, while file contents are stored in one or more free or commercial external storage solutions. On-demand, any file content can then be obtained with a <code>git-annex get</code> command from the external file storage.</p>"},{"location":"version-control/#git-submodules","title":"git submodules","text":"<p>Submodules allows to split the data in different repositories, while keeping everything under a single \"parent\" repository. It is very powerful, but difficult to use. Especially, using branches in  submodules make it complex to handle. However, this is the only tool listed here allowing to work with many files in a Git repository.</p>"},{"location":"version-control/#datalad","title":"DataLad","text":"<p>DataLad, builds upon git and git-annex. Like <code>git-annex</code>, it allows you to version control data and share it via third-party providers but simplifies and extends this functionality. In addition to sharing and version controlling large files; it allows recording, sharing, and using software environments, recording and re-executing commands or data analyses, and operating seamlessly across a hierarchy of repositories. You can record in hte provenance history where you got the data from, when you run which analysis on the data and where you stored the output by using a single command additionally to all the code-version control advantages of git. </p> <p>Please note: Powerful version control systems such as Git or even more DataLad take time to learn. There are a lot of tutorials, especially for learning Git, however, you will only get the gist if you simply start using it and also make some mistake on the way. But don't worry, if you make a mistake with version control systems it won't destroy your project or make you loose data. </p> <p>Another note: When you use version control systems, you need to make sure what you include in the version control and how you control access and sharing. </p> <p>Info</p> <p>Most of the content was copied from The Turing Way Handbook under a CC-BY 4.0 licence.</p>"},{"location":"version-control/#optionalreadingfurther-materials","title":"optional/reading/further materials","text":""},{"location":"version-control/#git","title":"Git","text":"<ul> <li>Beginner Git &amp; GitLab Tutorial</li> <li>NOWA Workshops</li> <li>NOWA School</li> <li>YouTube is full of Git/GitLab/GitHub videos for all kinds of levels and features!!! For example: Brainhack Git introduction or GitHub CI</li> <li>Git cheat sheet by gitlab or github</li> <li>Atlassian tutorials and cheat sheet</li> <li>Troubleshooting: Oh shit git or Dangit git (are the same, but the latter is without swearing)</li> <li>Git branching</li> <li>Git GUIs </li> <li>Advanced Git commands</li> <li>really, just type anything you want to know about Git in YouTube and you'll find a tutorial for it. I promise. </li> <li>Brainhack School Module: Git &amp; GitHub</li> </ul>"},{"location":"version-control/#datalad_1","title":"DataLad","text":"<ul> <li>DataLad Website</li> <li>DataLad Handbook</li> <li>DataLad@NeuroStars</li> <li>DataLad@YouTube</li> <li>Brainhack School Module: DataLad</li> </ul>"}]}